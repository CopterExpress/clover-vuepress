import{_ as r,r as n,o as h,c as d,a as t,b as a,w as l,F as c,d as e,e as i}from"./app.4f6a4bec.js";var p="/clover-vuepress/assets/Picture1.9ac8cb08.png",u="/clover-vuepress/assets/1.2fig.d1da5cee.png",m="/clover-vuepress/assets/1.3fig.9c3c312d.png",f="/clover-vuepress/assets/HLA.a732d51c.jpg",g="/clover-vuepress/assets/algorth_logic.5c2e2269.png",w="/clover-vuepress/assets/Maze1.b12abb74.jpg",b="/clover-vuepress/assets/hectortest1.2078e188.jpg",y="/clover-vuepress/assets/hectortest2.f51ac9d0.jpg",v="/clover-vuepress/assets/3.ce66d95b.jpeg";const k={},_=t("h1",{id:"dromap-the-indoor-mapping-drone",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#dromap-the-indoor-mapping-drone","aria-hidden":"true"},"#"),e(" DroMap: The Indoor Mapping Drone")],-1),T=e("CopterHack-2021"),S=e(", team: "),x=t("strong",null,"DroMap",-1),R=e(". E-mail: "),A=t("a",{href:"mailto:officialdromap@gmail.com"},"officialdromap@gmail.com",-1),L=e("."),M=i('<p>Team:</p><ul><li>Shouq AlQahtani</li><li>Ameena AlMansouri</li><li>Noof AlMarri</li></ul><h2 id="abstract" tabindex="-1"><a class="header-anchor" href="#abstract" aria-hidden="true">#</a> Abstract</h2><p>In the modern era, the world is witnessing a magnificent development in the field of architecture and interior design. Due to architectural development, the current measuring tools such as metal tapes and leaser meters became insufficient for assisting both architects and interior designers in taking the measurements for buildings and facilities. Because the accuracy of obtained readings depends on the professionalism of the users and the nature of these tools is unidirectional, the measurement taking process becomes less efficient in terms of time and labor. Since drones have played an essential role in revolutionizing the world of science and automation due to their use in a huge number of daily life applications, an introduction of indoor drones in the field of mapping and architecture is indispensable. Hence, the DroMap project proposes an autonomous indoor drone that can navigate autonomously and create a map of the indoor environment along the way. For the aforementioned purpose, a LiDAR sensor is used to collect data of the indoor place which is sent to a host computer. Afterward, simultaneous localization and mapping algorithm utilizes these data to pave the way for creating a 2-dimensional map. This autonomous indoor mapping drone system is not prone to inefficiency and human errors like in manual mapping and has the potential to take indoor mapping to the next level in the near future.</p><h2 id="motivation" tabindex="-1"><a class="header-anchor" href="#motivation" aria-hidden="true">#</a> Motivation</h2><h3 id="problem-statement" tabindex="-1"><a class="header-anchor" href="#problem-statement" aria-hidden="true">#</a> Problem Statement</h3><p>Architects lives are constantly in danger due to the nature of their work as they are supposed to enter buildings without knowing their structure, these dangers could potentially threaten their lives and can lead to many issues. According to [^1], there are 1.2 deaths per 100,000 architects and that job is ranked 19th among the most dangerous jobs in the United States. One example of a fatal accident is the accident of Bruno Travalja which happened in 2016, this architect fell from the 48th floor of a building while taking measurements. In addition to being dangerous, the process of mapping an indoor environment is time-consuming, especially in transferring the raw measurements into a 2-dimensional map [^2]. Therefore, the need for robot assistance in mapping and measurement taking processes reaches the peak. Autonomous indoor mapping using drones or robots is considered an important tool where the drone can reach different places which are inaccessible to humans due to space constraints or security reasons [^3].</p><p>The use of robots has increased dramatically within the past decade due to their enormous potential in both civil and architectural applications. Specifically, in designing and building robots for mapping enclosed buildings. Even though most of the works were implemented on unmanned ground vehicles (UGV), the current experimental use of unmanned Ground vehicles for indoor mapping suffers from a few shortcomings. Particularly, most implementations suffer from low performance regarding time consumption and have difficulty accessing narrow places. Since UGVs have limitations in terms of time consumption and navigation rigidity, in the DroMap project we decided to use drones as a replacement for UGVs to map indoor sites. This is because drones are unique in their ability to traverse any 3D interior space without any restrictive concerns regarding space architecture. Additionally, since these vehicles are not required to remain on the ground, aerial vehicles can fully explore the extent of the indoor space, regardless of their interiors. Furthermore, unmanned aerial vehicles can access difficult to reach areas.</p><p>A questionnaire was conducted for this project to study the need for an indoor mapping drone which involved 72 architects and interior designers. The following question was asked to have an estimate of the time taken by the targeted category for taking the measurements of a large building. According to the survey results, 61% of the sample consume more than 60 minutes to measure a large building. This shows that the measurement taking process is time-consuming.</p><p><img src="'+p+'" alt=""></p><h4 id="technical-challenges" tabindex="-1"><a class="header-anchor" href="#technical-challenges" aria-hidden="true">#</a> Technical Challenges</h4><ul><li>The positioning system calculates the odometry data based on the laser scanner poses. This might misestimate the drone\u2019s position with respect to the surroundings.</li><li>The LiDAR readings could be infinite if the distance between the LiDAR and the surrounding walls exceeds the LiDAR range.</li><li>The communication between the Raspberry Pi and the PC relies heavily on Wi-Fi. Therefore, any loss in the Wi-Fi signal would terminate the communication between the drone and PC.</li></ul><h4 id="non-technical-challenges" tabindex="-1"><a class="header-anchor" href="#non-technical-challenges" aria-hidden="true">#</a> Non-technical Challenges</h4><ul><li>The indoor environment could be full of obstacles, which impedes path planning process.</li><li>The mirrors, windows, and glass doors may affect the accuracy of the map as they are not detected correctly by the laser pulses.</li></ul><h3 id="project-significance" tabindex="-1"><a class="header-anchor" href="#project-significance" aria-hidden="true">#</a> Project Significance</h3><p>Measuring a room or a full building along with transforming the collected data to a full map is time-consuming and requires massive effort. DroMap helps architects and interior designers to measure and generate a fully constructed 2D map with less time and effort. To help us understand the problem better, we conducted a survey to assess the need for an indoor mapping drone. This project will provide a great advantage for architects and interior designers as it would save time and effort in the map construction process. In addition, it will assure great cooperation from both the computer and architecture fields.</p><p>Generally, the process of mapping an indoor environment is composed of two phases; the first phase is the measurement taking phase and the second phase is the map drawing phase. However, the project introduces another way to create a map that is faster and requires less effort; as the measurements of the surroundings will be taken by the system once it is activated and processed by Simultaneous Localization and Mapping algorithms (SLAM) for building and updating maps as well as positions of an unknown environment in robotics in real-time. According to the survey, 94% of the sample agreed that it would be useful to have a robotic based measuring tool. Therefore, the proposed solution will successfully assist architects and interior designers in mapping indoor areas.</p><p><img src="'+u+'" alt=""></p><p>Based on the conducted survey, accuracy is the most important characteristic to be satisfied with the project. The below figure demonstrates that 48 of the sample sizes care about having high accuracy. Moreover, the second most important feature to be reached is having a short scanning time, which highlights the importance of the project.</p><p><img src="'+m+'" alt=""></p><p>Furthermore, since the project employs multiple concepts related to indoor robots and indoor data processing, it can be extended to assist other fields in Qatar rather than the architecture field only. For example, this project could be a great step towards training drones to handle different tasks related to search and rescue such as entering buildings on fire or finding a missing person in indoor places. This will serve to develop more technologies to process the indoor data in various environments and conditions, also to develop drones that are capable to operate in indoor areas with different functionalities.</p><h2 id="proposed-solution" tabindex="-1"><a class="header-anchor" href="#proposed-solution" aria-hidden="true">#</a> Proposed solution</h2><p>DroMap project consists mainly of two major components: the drone and the drone add-on. The drone is responsible for the physical movement of the entire system. The drone add-ons consist of necessary sensors for mapping, path planning, and mounting equipment such as Raspberry Pi 4, RPLiDAR A1M8, Sonar, and range finder. The Raspberry Pi collects the data from the sensors. While the data is being collected by the Raspberry Pi, the Hector SLAM will process these data in real-time to formulate 2-dimensional maps. After that, the map will be sent wirelessly to a remote PC and visualized through RVIZ software tool.</p><p><img src="'+f+`" alt=""></p><h3 id="hardware-software-to-be-used" tabindex="-1"><a class="header-anchor" href="#hardware-software-to-be-used" aria-hidden="true">#</a> Hardware/software to be used</h3><h4 id="hardware-selection" tabindex="-1"><a class="header-anchor" href="#hardware-selection" aria-hidden="true">#</a> Hardware selection</h4><h5 id="coex-clover-drone-kit" tabindex="-1"><a class="header-anchor" href="#coex-clover-drone-kit" aria-hidden="true">#</a> COEX Clover Drone kit</h5><p>Clover is a complete STEM educational programmable drone kit which includes unassembled quadcopter with four propellers and open-source software.</p><ul><li>Limitless possibilities of a fully programmable drone (Open Source).</li><li>Drone can operate stably without GPS.</li><li>The Clover platform exploits the ROS framework.</li><li>Made especially for Indoor flights.</li></ul><h5 id="slamtec-rplidar-a1m8" tabindex="-1"><a class="header-anchor" href="#slamtec-rplidar-a1m8" aria-hidden="true">#</a> Slamtec RPLiDAR A1M8</h5><p>LiDAR is low cost 2D 360\xB0 12m scanning sensor.</p><ul><li>Omnidirectional Laser Range Scanner 360\xB0.</li><li>Compatible with ROS.</li><li>Very high sampling Rate 8k times, Considered as one of the Highest in the Current LiDAR industry.</li><li>Ideal for indoor Navigation and Localization using UAVS.</li></ul><h5 id="raspberry-pi-4-model-b" tabindex="-1"><a class="header-anchor" href="#raspberry-pi-4-model-b" aria-hidden="true">#</a> Raspberry Pi 4 Model B</h5><p>Raspberry Pi is a single-board computer which is used as a companion computer.</p><ul><li>Low energy consumption.</li><li>Connect the drone over Wi-Fi.</li><li>Responsible for flight autonomy.</li><li>Access and issue commands to peripherals.</li></ul><h5 id="vl53l1x-rangefinder-sensor" tabindex="-1"><a class="header-anchor" href="#vl53l1x-rangefinder-sensor" aria-hidden="true">#</a> VL53L1X RangeFinder Sensor</h5><p>Laser Ranging Sensor Module Rangefinder. One of the smallest time-of-flight 940 nm laser VCSEL. Measuring absolute range up to 4 meters.</p><ul><li>The Range Finder Optical Ranging sensor is an integrated sensor with embedded infrared, eye-safe laser, advanced filters and high-speed photon detection arrays.</li><li>Range finder Supports 400cm sensing range, suitable for many applications.</li></ul><h4 id="software-selection" tabindex="-1"><a class="header-anchor" href="#software-selection" aria-hidden="true">#</a> Software selection</h4><h5 id="robot-operating-system-ros" tabindex="-1"><a class="header-anchor" href="#robot-operating-system-ros" aria-hidden="true">#</a> Robot Operating System (ROS)</h5><p>A framework which runs on Linux operating system, and will be used as a firmware to control and monitor the system.</p><h5 id="coex-virtual-machine" tabindex="-1"><a class="header-anchor" href="#coex-virtual-machine" aria-hidden="true">#</a> COEX Virtual Machine</h5><p>A Linux operating system that has a pre-installed ROS along with some necessary dependencies and packages in addition to a pre-configured Gazebo environment.</p><h5 id="gazebo" tabindex="-1"><a class="header-anchor" href="#gazebo" aria-hidden="true">#</a> Gazebo</h5><p>The simulation tool that will be used to test and try different mapping and automation approaches.</p><h5 id="visual-studio-code" tabindex="-1"><a class="header-anchor" href="#visual-studio-code" aria-hidden="true">#</a> Visual Studio Code</h5><p>A text editor to write python scripts to program the drone.</p><h5 id="rviz" tabindex="-1"><a class="header-anchor" href="#rviz" aria-hidden="true">#</a> RVIZ</h5><p>A visualization tool to visualize the LiDAR readings.</p><h5 id="qgroundcontrol" tabindex="-1"><a class="header-anchor" href="#qgroundcontrol" aria-hidden="true">#</a> QGroundControl</h5><p>QGroundControl supports full flight control and mission planning for any MAVLink enabled drone.</p><h2 id="implementation" tabindex="-1"><a class="header-anchor" href="#implementation" aria-hidden="true">#</a> Implementation</h2><p>The implementation divided into two parts. The first part is to work on the simulation software, and the second part is to work on the physical hardware components. The simulation software helped us to have an estimation of how the system will work in the physical world. Through the simulation software, we were able to identify some implementation challenges and finding solutions for them. In addition, the simulation software gave us the opportunity to process the sensors data and test the sensors before testing them physically, which speeded up the process of working on the physical components and testing them. Moreover, it was found that the results obtained from the simulated components and the physical components were close to each other. In this section, we demonstrate the progress that happened in both the physical and the virtual worlds.</p><h3 id="the-simulation-software" tabindex="-1"><a class="header-anchor" href="#the-simulation-software" aria-hidden="true">#</a> The Simulation Software</h3><h4 id="the-mapping-algorithm" tabindex="-1"><a class="header-anchor" href="#the-mapping-algorithm" aria-hidden="true">#</a> The Mapping algorithm</h4><p>The Hector SLAM algorithm was selected in this project due to its high efficiency in mapping indoor environments, its ability to work with drones efficiently, and its facility to be integrated with the selected LiDAR sensor. Moreover, it consumes less power in handling some cases where the indoor environment is dynamic, and the obstacles are moving [^4]. Hector SLAM is an algorithm that is used widely in mapping unknown indoor environments. The algorithm is LiDAR-based, and it uses the Gaussian Newton equation to construct accurate maps from the laser scanner data [^5]. Moreover, this algorithm does not use any odometry data to estimate the robot\u2019s position with respect to its surroundings. Instead, the algorithm utilizes the difference in the laser scanner locations to calculate the odometry [^6]. This feature qualifies the Hector SLAM algorithm to work optimally with the unmanned aerial vehicles given that in most of the cases, the odometry data is calculated from processing the wheels motion and that is not the case with UAVs. In addition, the algorithm provides an accurate estimation of the robot\u2019s position with respect to its surroundings.</p><h4 id="the-exploration-algorithm" tabindex="-1"><a class="header-anchor" href="#the-exploration-algorithm" aria-hidden="true">#</a> The Exploration Algorithm</h4><p>The method used in this project to explore the indoor sites is selected to be the wall following algorithm due to its effectiveness and simplicity. The implementation of that algorithm can be summarized into three main functions which are: <code>left_side()</code> , <code>move_forward()</code>, and <code>take_stop_action()</code> which are represented in a while loop as following:</p><div class="custom-container tip"><p class="custom-container-title">TIP</p><p>The implementation of the wall following algorithm highly depends on the LiDAR used in the simulator which is Hokuyo laser scanner with 360 rotation angle and 720 readings per 32ms. However, the physical LiDAR used is RPLiDAR A1M8 which provides 360 readings per rotation.</p></div><div class="language-python ext-py"><pre class="language-python"><code><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    left_side<span class="token punctuation">(</span><span class="token punctuation">)</span>
    move_forward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    take_stop_action<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div class="language-python ext-py"><pre class="language-python"><code>left_side<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p>This function uses the LiDAR readings that are pointing exactly to the west of the drone, it measures how far the drone is from the left wall, then adjust the drone to it such that the drone is approximately 0.7 m away from the left wall. The reason of using 0.7 meters is because the drone has higher error that expected. Therefore, a while loop is used to ensure that the drone is far enough from the wall.</p><div class="language-python ext-py"><pre class="language-python"><code>move_forward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p>This function was implemented to safely move the drone forward without hitting a wall, or without skipping an outer corner. The logic behind this algorithm is that it uses the concept of the right angle, and multiple readings which correspond to different angles to measure the safe distances. The bellow flow chat demonstrates the logic of that algorithm in details.</p><img src="`+g+`" height="600" class="center zoom"><p>When the function ends, the drone will either stop after an inner corner, or an outer corner.</p><div class="language-python ext-py"><pre class="language-python"><code>take_stop_action<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p>This function handles two different situations:</p><ul><li>The first situation is that the drone may stop when it faces an inner corner, this can be detected by measuring the distance from the front wall, then compare the current distance of the left LiDAR reading with the previously recorded one, if the comparison showed that there is a small difference between these two readings, then this means that the drone must rotate to the right and continue its path.</li><li>The second situation is that the drone may stop when it detects an outer corner, the logic is exactly like the first situation except, that the drone must be away from the front wall (with distance greater than 1.5 meter). In addition, the difference between the current left LiDAR reading, and the previously recorded reading must be greater than 0.5 m. If this is the case, then the drone has stopped because of an outer corner. Therefore, the drone must rotate to the left and continue its path.</li></ul><p>The following video illustrates a ROS Simulation test on Wall Following Algorithm:</p>`,70),D=t("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/HgLHC0D76f8",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:""},null,-1),j=i('<h4 id="testing" tabindex="-1"><a class="header-anchor" href="#testing" aria-hidden="true">#</a> Testing</h4><p><img src="'+w+'" alt=""></p><p>The above figure demonstrates the drone exploring a maze autonomously while constructing a 2D map in real-time. The terminal shows the safe distances to move forward, these distancing where calculated using the aforementioned flowchart.</p><p>The following figures show a constructed 2D map of different environments.</p><p><img src="'+b+'" alt=""></p><p><img src="'+y+'" alt=""></p><p>The following video demonstrates an autonomous maze exploration with Hector SLAM responsible for constructing a 2D map:</p>',7),C=t("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/fxshlaoCnEc",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:""},null,-1),P=i('<h3 id="the-physical-hardware" tabindex="-1"><a class="header-anchor" href="#the-physical-hardware" aria-hidden="true">#</a> The physical hardware</h3><p>This section illustrates the progress done regarding the hardware components. The first step done was to establish a Wi-Fi communication between the Raspberry Pi and the remote PC. The second step was to install the hector SLAM and robot Localization packages in the Raspberry Pi to visualize the maps remotely.</p><h4 id="initial-setup" tabindex="-1"><a class="header-anchor" href="#initial-setup" aria-hidden="true">#</a> Initial Setup</h4><p>The drone is assembled and configured correctly to accomplish the autonomous mapping mission. The RPliDAR A1M8 and all other necessary sensors are mounted on the drone as shown in the figure bellow.</p><img src="'+v+'" width="400" class="center zoom"><p>To set up the drone ready for mapping, the raspberry pi image created by COEX was installed on the micro-SD card. COEX Raspberry Pi image, COEX pixracer image and COEX virtual machine were selected as they contain all the necessary tools and packages to work efficiently with clover platform. The installed platform is based on Raspbian operating system and ROS. After flashing the image on the SD, the next step is to connect clover to Wi-Fi.</p><h4 id="network-setup" tabindex="-1"><a class="header-anchor" href="#network-setup" aria-hidden="true">#</a> Network Setup</h4><p>The drone produces a map of an unknown indoor environment by sending data received from the sensors to a remote pc. The transmission takes place over a wireless channel to get a map in real-time. One of the essentials for DroMap is to setup the connection between the drone and the remote PC. In DroMap Project ROS network must satisfy the listed below requirements:</p><ol><li>There must be a full bidirectional communication between all the nodes.</li><li>Every component in the network must advertise its name.</li><li>In ROS network one of the components must be declared as the ROS master. Specifically, the ROS master is the drone (Clover-6064).</li><li>All ROS packages needed in the project, must use the ROS master.</li></ol><p>All these requirements are fulfilled in our design.</p><h4 id="required-packages" tabindex="-1"><a class="header-anchor" href="#required-packages" aria-hidden="true">#</a> Required packages</h4><p>After the installation of ROS, the drone was ready to install RPLidar ROS package and Hector SLAM. These packages are installed by cloning them in a catkin workspace src folder. Then build them by running catkin build. The following commands were entered in the terminal show the process of installing RPLidar package and hector SLAM in raspberry pi. The <code>rplidar_ros</code> package is responsible for retrieving the RPLidar data and hector SLAM package is responsible for building maps. <code>rplidar_ros</code> and <code>hector_slam</code> packages ware installed from GitHub.</p><h4 id="testing-1" tabindex="-1"><a class="header-anchor" href="#testing-1" aria-hidden="true">#</a> Testing</h4><p>The testing phase was divided into several stages in order to test the sensor and the SLAM algorithm in several closed places. This makes it possible to identify obstacles and risks that may face us in the future.</p><p>We did several of the following elementary tests:</p><ul><li>Firstly, we flew the drone to obtain maps using Hector Mapping with the remote control.</li><li>Secondly, we have moved to the automation stage of implementing the codes applied in the simulator.</li><li>Finally, From here we did some tests, for example, the drone flies to the wall, and then lands after getting a wall reading. And tests are still going on for a fully automatic flight.</li></ul><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2>',17),O=e("[^1]: \u201CThe 20 deadliest jobs in America, ranked,\u201D CBS News. "),z={href:"https://www.cbsnews.com/pictures/the-20-deadliest-jobs-in-america-ranked/4/",target:"_blank",rel:"noopener noreferrer"},I=e("https://www.cbsnews.com/pictures/the-20-deadliest-jobs-in-america-ranked/4/"),E=e(". [^2]: A. Kovalchenko, \u201CHow To Carry Out a Survey and Site Measure,\u201D 2012. "),H={href:"https://essenziale-hd.com/2012/10/28/how-to-carry-out-a-survey-and-site-measure/",target:"_blank",rel:"noopener noreferrer"},q=e("https://essenziale-hd.com/2012/10/28/how-to-carry-out-a-survey-and-site-measure/"),F=e(". [^3]: D. H\xE4hnel, W. Burgard, and S. Thurn, \u201CLearning compact 3D models of indoor and outdoor environments with a mobile robot,\u201D Rob. Auton. Syst., vol. 44, no. 1, pp. 15\u201327, 2003, doi: 10.1016/S0921-8890(03)00007-1. [^4]: M. Eliwa, A. Adham, I. Sami, and M. Eldeeb, \u201CA critical comparison between Fast and Hector SLAM algorithms,\u201D / REST J. Emerg. trends Model. Manuf., vol. 3, no. 2, pp. 44\u201349, 2017, [Online]. Available: "),V={href:"http://www.restpublisher.com/journals/jemm",target:"_blank",rel:"noopener noreferrer"},G=e("www.restpublisher.com/journals/jemm"),B=e(". [^5]: J. M. Santos, D. Portugal, and R. P. Rocha, \u201CAn evaluation of 2D SLAM techniques available in Robot Operating System,\u201D 2013 IEEE Int. Symp. Safety, Secur. Rescue Robot. SSRR 2013, 2013, doi: 10.1109/SSRR.2013.6719348. [^6]: H. Gossett, \u201CBuilding an Autonomous Indoor Drone System,\u201D University of Mississippi, 2018.");function N(W,U){const s=n("RouterLink"),o=n("ExternalLinkIcon");return h(),d(c,null,[_,t("p",null,[a(s,{to:"/en/copterhack2021.html"},{default:l(()=>[T]),_:1}),S,x,R,A,L]),M,D,j,C,P,t("p",null,[O,t("a",z,[I,a(o)]),E,t("a",H,[q,a(o)]),F,t("a",V,[G,a(o)]),B])],64)}var Q=r(k,[["render",N],["__file","dromap.html.vue"]]);export{Q as default};
